# Pipeline ETL de Procesamiento de Entregas

Este proyecto contiene un pipeline ETL completo desarrollado en PySpark para la prueba tÃ©cnica. La soluciÃ³n procesa datos de entregas de productos, aplicando una serie de transformaciones, validaciones de calidad y reglas de negocio, todo controlado de manera centralizada a travÃ©s de un archivo de configuraciÃ³n.

## Entorno de Desarrollo y Dependencias

La soluciÃ³n fue desarrollada y probada exitosamente en el siguiente entorno de Databricks:

- **Databricks Runtime:** 17.0
- **Apache Spark:** 4.0.0
- **Python:** 3.11 (o la versiÃ³n que corresponda a tu DBR)
- **LibrerÃ­as Principales:** `pyspark`, `omegaconf`, `PyYAML`

### Estructura del Repositorio

```
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml          # Archivo de configuraciÃ³n (opcional, ya que el notebook lo genera)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ entregas_productos_prueba.csv # Datos de entrada
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 20250723_etl_entrega_productos.ipynb # El notebook principal
â”œâ”€â”€ LICENSE                  # MIT License
â”œâ”€â”€ README.md                # Este archivo
â””â”€â”€ requirements.txt         # Dependencias del proyecto
```

## InstalaciÃ³n

1.  Clona este repositorio en tu mÃ¡quina local.
2.  Para configurar un entorno local de prueba, crea un entorno virtual e instala las dependencias:
    ```bash
    python -m venv venv
    source venv/bin/activate  # En Windows: venv\Scripts\activate
    pip install -r requirements.txt
    ```

## CÃ³mo Ejecutar en Databricks

1.  **Subir los Datos:** Sube el archivo `data/entregas_productos_prueba.csv` al Databricks File System (DBFS) en la siguiente ruta:
    ```
    dbfs:/FileStore/etl_pipeline_final/bronze/
    ```

2.  **Importar el Notebook:** Importa el archivo `notebooks/Prueba_Tecnica_Final.py` a tu Workspace de Databricks. La plataforma lo convertirÃ¡ automÃ¡ticamente en un notebook con celdas.

3.  **Ejecutar el Notebook:** Abre el notebook importado, asegÃºrate de que estÃ© adjunto a un clÃºster con un Runtime compatible (DBR 17.0 o similar) y ejecuta todas las celdas en orden.

## DescripciÃ³n de la SoluciÃ³n

El notebook estÃ¡ estructurado en cuatro fases principales que demuestran el ciclo de vida completo del procesamiento de datos:

1.  **ConfiguraciÃ³n y Herramientas:** Se prepara el entorno, se genera dinÃ¡micamente el archivo de configuraciÃ³n `config.yaml` y se cargan las librerÃ­as.
2.  **DefiniciÃ³n del Pipeline:** Se encapsula toda la lÃ³gica ETL en una clase `DataPipeline` para un cÃ³digo limpio y mantenible.
3.  **EjecuciÃ³n del Pipeline:** Se orquesta la ejecuciÃ³n para mÃºltiples escenarios (diferentes paÃ­ses y fechas) para probar la flexibilidad de la soluciÃ³n.
4.  **AuditorÃ­a de Resultados:** Se verifica que los datos procesados y los datos en cuarentena se hayan escrito correctamente en las carpetas de salida.

## DescripciÃ³n GrÃ¡fica de la SoluciÃ³n

```mermaid
flowchart TD
    %% DefiniciÃ³n de clases para styling
    classDef inputClass fill:#e1f5fe,stroke:#0288d1,stroke-width:2px,color:#000
    classDef processClass fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    classDef outputClass fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000
    classDef decisionClass fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    
    %% Subgrafo de Entradas
    subgraph INPUT [" ğŸ“¥ ENTRADAS DEL PIPELINE "]
        direction TB
        A["ğŸ“„ entregas_productos_prueba.csv<br/>ğŸ”¹ <b>Datos Crudos de Entregas</b><br/>ğŸ”¹ Formato: CSV"]
        B["âš™ï¸ config.yaml<br/>ğŸ”¹ <b>Reglas y ParÃ¡metros</b><br/>ğŸ”¹ ConfiguraciÃ³n de ValidaciÃ³n"]
    end
    
    %% Subgrafo de Procesamiento
    subgraph PROCESS [" âš¡ PROCESO ETL - PySpark/Databricks "]
        direction TB
        C["ğŸ”„ <b>1. Lectura y Carga de Datos</b><br/>ğŸ“Š Spark DataFrame Loading"]
        D{"ğŸ” <b>2. ValidaciÃ³n y Calidad</b><br/>ğŸ“‹ Data Quality Checks<br/>âœ… Reglas de Negocio"}
        E["ğŸ”§ <b>3. Transformaciones de Negocio</b><br/>ğŸ“… Filtrado por fecha/paÃ­s<br/>ğŸ“ NormalizaciÃ³n de unidades<br/>ğŸ“¦ ClasificaciÃ³n de entregas<br/>ğŸ”¤ EstandarizaciÃ³n de columnas"]
    end
    
    %% Subgrafo de Salidas
    subgraph OUTPUT [" ğŸ“¤ SALIDAS GENERADAS "]
        direction TB
        F["ğŸ—„ï¸ <b>Datos Procesados (Silver)</b><br/>ğŸ“ Formato: Parquet<br/>ğŸ—‚ï¸ Particionado por fecha_proceso<br/>âœ… Listos para AnÃ¡lisis"]
        G["âš ï¸ <b>Registros en Cuarentena</b><br/>ğŸ“ Formato: Parquet<br/>ğŸ“ Contiene motivo del rechazo<br/>ğŸ” Para RevisiÃ³n Manual"]
    end
    
    %% Conexiones del Flujo Principal
    A --> C
    B --> C
    C --> D
    D -->|"âœ… Registros VÃ¡lidos"| E
    D -->|"âŒ Registros InvÃ¡lidos"| G
    E --> F
    
    %% Aplicar clases de styling
    class A,B inputClass
    class C,E processClass
    class D decisionClass
    class F,G outputClass
    
    %% Styling de subgrafos
    style INPUT fill:#f8f9fa,stroke:#343a40,stroke-width:3px
    style PROCESS fill:#f8f9fa,stroke:#343a40,stroke-width:3px
    style OUTPUT fill:#f8f9fa,stroke:#343a40,stroke-width:3px
```
