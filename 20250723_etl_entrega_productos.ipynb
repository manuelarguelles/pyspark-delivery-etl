{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ffd8b55-0461-4e67-8188-5254323fb981",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Prueba Técnica: Pipeline de Procesamiento de Datos con PySpark y OmegaConf\n",
    "\n",
    "Este notebook implementa un pipeline ETL completo para procesar datos de entregas de productos. El proceso es totalmente parametrizable a través de un archivo de configuración `config.yaml`, utiliza PySpark para las transformaciones y está diseñado para ser robusto y auditable.\n",
    "\n",
    "**Entorno de Ejecución:**\n",
    "*   **Databricks Runtime:** 17.0\n",
    "*   **Apache Spark:** 4.0.0\n",
    "*   **OmegaConf:** 2.3.0\n",
    "\n",
    "**El flujo de trabajo se divide en las siguientes fases:**\n",
    "1.  **Configuración y Herramientas:** Se definen y cargan la configuración y las funciones de utilidad.\n",
    "2.  **Definición del Pipeline:** Se define la lógica ETL principal encapsulada en una clase.\n",
    "3.  **Ejecución del Pipeline:** Se ejecutan varios escenarios de prueba para demostrar la flexibilidad del pipeline.\n",
    "4.  **Auditoría de Resultados:** Se verifican los datos escritos en la capa final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94956f54-6d42-437c-bc52-fb5ae013f619",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fase 0: Configuración y Herramientas\n",
    "\n",
    "En esta fase inicial, importamos las librerías necesarias, establecemos la estructura de directorios y generamos/cargamos la configuración `config.yaml` que gobernará todo el proceso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "359710aa-ca93-491a-aa6c-c6407760f259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 0.1 - Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f930612c-0507-48b6-9d11-c7608e51248d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Librerías Principales de PySpark ---\n",
    "from pyspark.sql import SparkSession, DataFrame # Para iniciar Spark y manejar su estructura de datos principal (el DataFrame).\n",
    "from pyspark.sql.functions import col, lit, when, to_date, current_timestamp, sum as spark_sum, count, isnull, concat_ws # Herramientas para manipular y transformar columnas.\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, DateType, TimestampType, StructType, StructField # Para definir la estructura (schema) de nuestros datos.\n",
    "\n",
    "# --- Librerías para Configuración y Sistema de Archivos ---\n",
    "from omegaconf import OmegaConf # Ayuda a leer y manejar nuestro archivo de configuración YAML de forma sencilla.\n",
    "import yaml # La librería base para trabajar con archivos YAML en Python.\n",
    "from pyspark.dbutils import DBUtils # Utilidades de Databricks para interactuar con su sistema de archivos (DBFS).\n",
    "\n",
    "# --- Librerías Auxiliares de Python ---\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Inicialización de utilidades de Databricks\n",
    "dbutils = DBUtils(spark)\n",
    "\n",
    "print(\"✅ Librerías y utilidades cargadas correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6d1e64e-433e-4626-89af-2c7b5443fd50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Versión de Apache Spark que está corriendo en el clúster\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ca88e4d-1278-43bf-9211-83a522a83543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Versión de la librería OmegaConf previamente instalada en el clúster\n",
    "import omegaconf\n",
    "print(omegaconf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "602d6b3a-b601-4acb-9bde-a3dd63cc9419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 0.2 - Creación de la Estructura de Directorios\n",
    "\n",
    "Aseguramos que la estructura de carpetas necesaria para el pipeline exista en DBFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f9bdf64-8369-4361-b64c-20e942899568",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definimos la ruta base para todo el proyecto\n",
    "base_path = \"/FileStore/etl_pipeline_final\"\n",
    "\n",
    "# Lista de directorios a crear\n",
    "directories = [\n",
    "    f\"{base_path}/config\",\n",
    "    f\"{base_path}/bronze\",      # Para los datos de entrada (crudos)\n",
    "    f\"{base_path}/silver\",      # Para los datos procesados y limpios\n",
    "    f\"{base_path}/quarantine\",  # Para los registros que no pasen las validaciones\n",
    "    f\"{base_path}/logs\"\n",
    "]\n",
    "\n",
    "# Creamos los directorios\n",
    "for directory in directories:\n",
    "    dbutils.fs.mkdirs(directory)\n",
    "\n",
    "print(f\"✅ Estructura de directorios creada o verificada en: {base_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dcdc5cf-cf02-4ccb-9676-fab2a89b637f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 0.3 - Generación Dinámica del Archivo `config.yaml`\n",
    "\n",
    "Para esta demostración, el archivo `config.yaml` se genera y guarda directamente desde el notebook. En un entorno productivo, este archivo existiría de forma independiente en el repositorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d44f1a5d-f5d7-48c8-a0ad-4680bd500701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Contenido completo del archivo de configuración\n",
    "yaml_config_content = f\"\"\"\n",
    "# =============================================================================\n",
    "# Configuración Central para el Pipeline ETL de Entregas\n",
    "# =============================================================================\n",
    "\n",
    "# --- Rutas del Sistema de Archivos ---\n",
    "paths:\n",
    "  base: \"{base_path}\"\n",
    "  input_data: \"${{paths.base}}/bronze/entregas_productos_prueba.csv\"\n",
    "  output_processed: \"${{paths.base}}/silver/entregas_productos_procesados\"\n",
    "  output_quarantine: \"${{paths.base}}/quarantine/entregas_productos_invalidos\"\n",
    "\n",
    "# --- Parámetros de Ejecución ---\n",
    "# Estos son los parámetros que típicamente se cambian en cada ejecución.\n",
    "run_params:\n",
    "  start_date: \"2025-01-01\"   # Formato: YYYY-MM-DD\n",
    "  end_date: \"2025-06-30\"     # Formato: YYYY-MM-DD\n",
    "  country_code: \"GT\"         # Código de país a procesar (ej: GT, PE, SV)\n",
    "\n",
    "# --- Reglas de Negocio y Transformaciones ---\n",
    "business_logic:\n",
    "  unit_conversion:\n",
    "    CS: 20\n",
    "    ST: 1\n",
    "  \n",
    "  delivery_types:\n",
    "    routine: [\"ZPRE\", \"ZVE1\"]\n",
    "    bonus: [\"Z04\", \"Z05\"]\n",
    "\n",
    "# --- Controles de Calidad de Datos ---\n",
    "data_quality:\n",
    "  allow_duplicates: false # Si es 'true', no se eliminarán filas duplicadas.\n",
    "  \n",
    "  # Columnas que no pueden ser nulas o vacías para que un registro sea válido.\n",
    "  required_columns: [\"pais\", \"fecha_proceso\", \"material\", \"cantidad\", \"unidad\", \"tipo_entrega\"]\n",
    "\n",
    "# --- Estructura y Enriquecimiento del Resultado Final ---\n",
    "schema_and_enrichment:\n",
    "  # Mapeo para estandarizar los nombres de las columnas.\n",
    "  column_rename_map:\n",
    "    pais: \"codigo_pais\"\n",
    "    fecha_proceso: \"fecha_proceso\"\n",
    "    transporte: \"id_transporte\"\n",
    "    ruta: \"id_ruta\"\n",
    "    material: \"id_material\"\n",
    "    precio: \"precio\"\n",
    "    cantidad: \"cantidad_original\"\n",
    "    unidad: \"unidad_original\"\n",
    "  \n",
    "  # Mapeo para añadir el nombre completo del país.\n",
    "  country_mapping:\n",
    "    GT: \"Guatemala\"\n",
    "    PE: \"Perú\"\n",
    "    EC: \"Ecuador\"\n",
    "    SV: \"El Salvador\"\n",
    "    HN: \"Honduras\"\n",
    "    JM: \"Jamaica\"\n",
    "\n",
    "# --- Configuración de Salida ---\n",
    "output:\n",
    "  partition_column: \"fecha_proceso\"\n",
    "  format: \"parquet\"\n",
    "  mode: \"overwrite\"\n",
    "\"\"\"\n",
    "\n",
    "# Guardamos el contenido en un archivo en DBFS\n",
    "config_path_dbfs = f\"{base_path}/config/config.yaml\"\n",
    "dbutils.fs.put(config_path_dbfs, yaml_config_content, overwrite=True)\n",
    "\n",
    "print(f\"✅ Archivo config.yaml generado y guardado en: {config_path_dbfs}\")\n",
    "print(\"\\n--- Vista Previa del Archivo ---\")\n",
    "print(dbutils.fs.head(config_path_dbfs, 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2b22623-c4b9-414c-bac6-66ae1afebd27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 0.4 - Carga de la Configuración\n",
    "\n",
    "Leemos el archivo `config.yaml` y lo cargamos en un objeto de OmegaConf para un fácil acceso durante el pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d93e1dc0-0d37-477c-9e1b-9c15868eeac5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def dbfs_to_local_path(dbfs_path: str) -> str:\n",
    "    \"\"\"Convierte una ruta DBFS a una ruta de sistema de archivos local.\"\"\"\n",
    "    return dbfs_path.replace(\"dbfs:\", \"/dbfs\")\n",
    "\n",
    "try:\n",
    "    # Convertimos la ruta DBFS a una que OmegaConf pueda leer\n",
    "    local_config_path = dbfs_to_local_path(config_path_dbfs)\n",
    "    \n",
    "    # Cargamos la configuración en una variable global para el notebook\n",
    "    CONFIG = OmegaConf.load(local_config_path)\n",
    "    \n",
    "    print(\"✅ Configuración cargada exitosamente en la variable 'CONFIG'.\")\n",
    "    print(f\"País a procesar por defecto: {CONFIG.run_params.country_code}\")\n",
    "    print(f\"Ruta de entrada: {CONFIG.paths.input_data}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al cargar la configuración: {e}\")\n",
    "    # Detenemos la ejecución si la configuración no se puede cargar\n",
    "    dbutils.notebook.exit(\"Fallo en la carga de configuración.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7037360-6c29-4234-b589-ac53743f939b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fase 1: Definición del Pipeline (Lógica ETL)\n",
    "\n",
    "Encapsulamos toda la lógica de extracción, transformación y carga en una clase `DataPipeline`. Esto mantiene el código organizado, reutilizable y fácil de entender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac777a64-61ce-4e5a-9673-cab728f6e992",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DataPipeline:\n",
    "    \"\"\"\n",
    "    Clase que contiene toda la lógica para procesar los datos de entregas.\n",
    "    \"\"\"\n",
    "    def __init__(self, spark_session: SparkSession, config: OmegaConf):\n",
    "        \"\"\"\n",
    "        Inicializa el pipeline con la sesión de Spark y la configuración.\n",
    "        \"\"\"\n",
    "        self.spark = spark_session\n",
    "        self.config = config\n",
    "        \n",
    "        # Definimos el esquema de entrada para una lectura de datos más robusta\n",
    "        self.input_schema = StructType([\n",
    "            StructField(\"pais\", StringType(), True),\n",
    "            StructField(\"fecha_proceso\", StringType(), True), # Leemos como string para manejar el formato yyyyMMdd\n",
    "            StructField(\"transporte\", StringType(), True),\n",
    "            StructField(\"ruta\", StringType(), True),\n",
    "            StructField(\"tipo_entrega\", StringType(), True),\n",
    "            StructField(\"material\", StringType(), True),\n",
    "            StructField(\"precio\", DoubleType(), True),\n",
    "            StructField(\"cantidad\", DoubleType(), True),\n",
    "            StructField(\"unidad\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "    def read_source_data(self) -> DataFrame:\n",
    "        \"\"\"Lee los datos desde la ruta de entrada especificada en la configuración.\"\"\"\n",
    "        print(f\"\uD83D\uDCD6 Leyendo datos desde {self.config.paths.input_data}...\")\n",
    "        df = self.spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .schema(self.input_schema) \\\n",
    "            .csv(self.config.paths.input_data)\n",
    "        print(f\"   Total de registros leídos: {df.count()}\")\n",
    "        return df\n",
    "\n",
    "    def clean_and_quarantine_data(self, df: DataFrame) -> (DataFrame, DataFrame):\n",
    "        \"\"\"\n",
    "        Aplica reglas de calidad, elimina duplicados y separa los datos válidos\n",
    "        de los inválidos (cuarentena).\n",
    "        \"\"\"\n",
    "        print(\"\uD83E\uDDF9 Aplicando limpieza y separando datos inválidos...\")\n",
    "        \n",
    "        # 1. Manejo de duplicados\n",
    "        if not self.config.data_quality.allow_duplicates:\n",
    "            initial_count = df.count()\n",
    "            df = df.dropDuplicates()\n",
    "            print(f\"   - Duplicados eliminados: {initial_count - df.count()} filas.\")\n",
    "\n",
    "        # 2. Identificar registros inválidos\n",
    "        reasons = []\n",
    "        # Chequeo de nulos en columnas requeridas\n",
    "        for col_name in self.config.data_quality.required_columns:\n",
    "            reasons.append(when(col(col_name).isNull(), f\"Columna '{col_name}' es nula\"))\n",
    "        \n",
    "        # Chequeo de tipos de entrega válidos\n",
    "        valid_delivery_types = self.config.business_logic.delivery_types.routine + self.config.business_logic.delivery_types.bonus\n",
    "        reasons.append(when(~col(\"tipo_entrega\").isin(valid_delivery_types), \"Tipo de entrega no válido\"))\n",
    "\n",
    "        # Chequeo de material vacío\n",
    "        reasons.append(when(col(\"material\") == \"\", \"Material está vacío\"))\n",
    "\n",
    "        # Concatenar todas las razones de rechazo\n",
    "        df_with_reason = df.withColumn(\"rejection_reason\",\n",
    "            concat_ws(\"; \", *[r for r in reasons if r is not None])\n",
    "        )\n",
    "        # Un registro es válido si no tiene razón de rechazo\n",
    "        df_with_validity = df_with_reason.withColumn(\"is_valid\", col(\"rejection_reason\") == \"\")\n",
    "\n",
    "        # 3. Separar en dos DataFrames\n",
    "        df_clean = df_with_validity.filter(col(\"is_valid\")).drop(\"is_valid\", \"rejection_reason\")\n",
    "        df_quarantine = df_with_validity.filter(~col(\"is_valid\")).select(df.columns + [\"rejection_reason\"])\n",
    "\n",
    "        print(f\"   - Registros válidos para procesar: {df_clean.count()}\")\n",
    "        print(f\"   - Registros enviados a cuarentena: {df_quarantine.count()}\")\n",
    "        \n",
    "        return df_clean, df_quarantine\n",
    "\n",
    "    def transform_data(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Aplica todas las transformaciones de negocio al DataFrame limpio.\"\"\"\n",
    "        print(\"✨ Aplicando transformaciones de negocio...\")\n",
    "\n",
    "        # 1. Convertir fecha al formato correcto y filtrar por rango\n",
    "        df_transformed = df.withColumn(\"fecha_proceso_dt\", to_date(col(\"fecha_proceso\"), \"yyyyMMdd\"))\n",
    "        \n",
    "        df_filtered = df_transformed.filter(\n",
    "            (col(\"fecha_proceso_dt\") >= lit(self.config.run_params.start_date)) &\n",
    "            (col(\"fecha_proceso_dt\") <= lit(self.config.run_params.end_date)) &\n",
    "            (col(\"pais\") == lit(self.config.run_params.country_code))\n",
    "        )\n",
    "        print(f\"   - Registros después de filtrar por fecha y país: {df_filtered.count()}\")\n",
    "\n",
    "        # 2. Renombrar columnas\n",
    "        for old_name, new_name in self.config.schema_and_enrichment.column_rename_map.items():\n",
    "            df_filtered = df_filtered.withColumnRenamed(old_name, new_name)\n",
    "        print(\"   - Columnas renombradas según estándar.\")\n",
    "\n",
    "        # 3. Normalizar unidades\n",
    "        unit_map = self.config.business_logic.unit_conversion\n",
    "        df_filtered = df_filtered.withColumn(\"cantidad_total_unidades\",\n",
    "            when(col(\"unidad_original\") == \"CS\", col(\"cantidad_original\") * unit_map.CS)\n",
    "            .when(col(\"unidad_original\") == \"ST\", col(\"cantidad_original\") * unit_map.ST)\n",
    "            .otherwise(col(\"cantidad_original\"))\n",
    "        )\n",
    "        print(\"   - Cantidades normalizadas a unidades.\")\n",
    "\n",
    "        # 4. Clasificar tipo de entrega\n",
    "        delivery_map = self.config.business_logic.delivery_types\n",
    "        df_filtered = df_filtered \\\n",
    "            .withColumn(\"es_entrega_rutina\", col(\"tipo_entrega\").isin(delivery_map.routine).cast(\"boolean\")) \\\n",
    "            .withColumn(\"es_entrega_bonificacion\", col(\"tipo_entrega\").isin(delivery_map.bonus).cast(\"boolean\"))\n",
    "        print(\"   - Tipos de entrega clasificados.\")\n",
    "\n",
    "        # 5. Enriquecer con nombre del país\n",
    "        country_map = self.config.schema_and_enrichment.country_mapping\n",
    "        mapping_expr = when(col(\"codigo_pais\") == \"GT\", country_map.GT)\n",
    "        for code, name in country_map.items():\n",
    "            mapping_expr = mapping_expr.when(col(\"codigo_pais\") == code, name)\n",
    "        df_filtered = df_filtered.withColumn(\"nombre_pais\", mapping_expr.otherwise(\"Desconocido\"))\n",
    "        print(\"   - Nombre del país añadido.\")\n",
    "\n",
    "        # 6. Añadir metadatos de ejecución\n",
    "        df_final = df_filtered.withColumn(\"fecha_ejecucion_etl\", current_timestamp())\n",
    "        print(\"   - Metadatos de ejecución añadidos.\")\n",
    "\n",
    "        return df_final\n",
    "        \n",
    "    def write_data(self, df: DataFrame, path: str):\n",
    "        \"\"\"Escribe un DataFrame en la ruta especificada, particionado por fecha.\"\"\"\n",
    "        if df.count() == 0:\n",
    "            print(f\"⚠️ No hay datos para escribir en {path}. Se omite la escritura.\")\n",
    "            return\n",
    "\n",
    "        print(f\"\uD83D\uDCBE Escribiendo {df.count()} registros en {path}...\")\n",
    "        \n",
    "        df.write \\\n",
    "          .mode(self.config.output.mode) \\\n",
    "          .partitionBy(self.config.output.partition_column) \\\n",
    "          .format(self.config.output.format) \\\n",
    "          .save(path)\n",
    "        \n",
    "        print(\"   - Escritura completada.\")\n",
    "\n",
    "print(\"✅ Clase DataPipeline definida correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed4b8696-a03a-42a9-b61e-7d8e6eae88a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fase 2: Orquestación y Ejecución del Pipeline\n",
    "\n",
    "Aquí definimos una función `run_pipeline` que orquesta todos los pasos: instancia la clase `DataPipeline`, llama a sus métodos en orden y maneja la escritura de los datos procesados y de cuarentena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5c4b0a5-3ba2-4be1-b8f1-ec637555e014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_pipeline(config: OmegaConf, **overrides):\n",
    "    \"\"\"\n",
    "    Orquesta la ejecución completa del pipeline para una configuración dada.\n",
    "    Permite sobrescribir parámetros de la configuración para ejecuciones específicas.\n",
    "    \"\"\"\n",
    "    run_config = config.copy()\n",
    "    if overrides:\n",
    "        # Si se pasan parámetros extra (ej: pais=\"PE\"), se fusionan con la config\n",
    "        print(f\"\uD83D\uDD29 Sobrescribiendo configuración con: {overrides}\")\n",
    "        override_conf = OmegaConf.create({\"run_params\": overrides})\n",
    "        run_config = OmegaConf.merge(run_config, override_conf)\n",
    "\n",
    "    execution_name = f\"Ejecución para {run_config.run_params.country_code} ({run_config.run_params.start_date} a {run_config.run_params.end_date})\"\n",
    "    print(f\"\\n\uD83D\uDE80 --- Iniciando: {execution_name} --- \uD83D\uDE80\")\n",
    "    \n",
    "    # 1. Instanciar el pipeline\n",
    "    pipeline = DataPipeline(spark, run_config)\n",
    "    \n",
    "    # 2. Leer datos\n",
    "    df_raw = pipeline.read_source_data()\n",
    "    \n",
    "    # 3. Limpiar y separar\n",
    "    df_clean, df_quarantine = pipeline.clean_and_quarantine_data(df_raw)\n",
    "    \n",
    "    # 4. Transformar\n",
    "    df_transformed = pipeline.transform_data(df_clean)\n",
    "    \n",
    "    # 5. Inspección Pre-Escritura\n",
    "    print(\"\\n\uD83D\uDD75️ --- Inspección Pre-Escritura (Datos Procesados) ---\")\n",
    "    if df_transformed.count() > 0:\n",
    "        df_transformed.select(\n",
    "            \"codigo_pais\", \"fecha_proceso\", \"id_material\", \"cantidad_total_unidades\", \n",
    "            \"es_entrega_rutina\", \"es_entrega_bonificacion\", \"nombre_pais\"\n",
    "        ).show(5, truncate=False)\n",
    "    else:\n",
    "        print(\"No hay datos procesados para mostrar.\")\n",
    "\n",
    "    # 6. Escribir resultados\n",
    "    pipeline.write_data(df_transformed, run_config.paths.output_processed)\n",
    "    pipeline.write_data(df_quarantine, run_config.paths.output_quarantine)\n",
    "    \n",
    "    print(f\"\uD83C\uDFC1 --- Finalizado: {execution_name} --- \uD83C\uDFC1\\n\")\n",
    "\n",
    "\n",
    "print(\"✅ Función de orquestación `run_pipeline` definida.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6df36fdd-44f3-4156-8ed7-55f72e00e1d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fase 3: Ejecuciones de Prueba\n",
    "\n",
    "Ahora usamos la función `run_pipeline` para ejecutar el proceso con diferentes configuraciones, demostrando su parametrización y flexibilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df565e8c-b2cc-4188-927a-bcb890c0d030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3.1 - Ejecución con Configuración por Defecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f7070ad-94e4-470c-9929-9043cc0c6555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ejecutar el pipeline con la configuración cargada por defecto desde el config.yaml (País: GT)\n",
    "run_pipeline(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef86b5d8-5707-4606-acdb-962bf7326d6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3.2 - Ejecución para Múltiples Escenarios\n",
    "\n",
    "Se define una lista de escenarios para procesar diferentes países y rangos de fechas en un solo bucle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91d257f8-eb06-47ad-b1dc-0ef758ffa439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scenarios = [\n",
    "    {\"country_code\": \"PE\", \"start_date\": \"2025-01-01\", \"end_date\": \"2025-01-31\"},\n",
    "    {\"country_code\": \"EC\", \"start_date\": \"2025-02-01\", \"end_date\": \"2025-02-28\"},\n",
    "    {\"country_code\": \"SV\", \"start_date\": \"2025-03-01\", \"end_date\": \"2025-03-31\"},\n",
    "    {\"country_code\": \"HN\", \"start_date\": \"2025-03-01\", \"end_date\": \"2025-03-31\"},\n",
    "    {\"country_code\": \"JM\", \"start_date\": \"2025-06-01\", \"end_date\": \"2025-06-30\"},\n",
    "]\n",
    "\n",
    "for scenario in scenarios:\n",
    "    run_pipeline(CONFIG, **scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20c2b81e-aa5e-4325-a175-5ab411f12115",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fase 4: Auditoría de Resultados\n",
    "\n",
    "Finalmente, verificamos los resultados escritos en el sistema de archivos. Creamos una función de auditoría para inspeccionar las particiones y leer una muestra de los datos guardados, tanto de los procesados como de los que están en cuarentena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e9b9022-ddbb-4b34-a3e3-519563da3939",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def audit_output_folder(path: str, description: str):\n",
    "    \"\"\"\n",
    "    Lista las particiones de una ruta de salida y muestra una muestra\n",
    "    de la primera partición que encuentre.\n",
    "    \"\"\"\n",
    "    print(f\"\\n\uD83D\uDD0E --- Auditoría de la Carpeta: {description} ---\")\n",
    "    print(f\"Ruta: {path}\")\n",
    "    \n",
    "    try:\n",
    "        partitions = dbutils.fs.ls(path)\n",
    "        dir_partitions = [p for p in partitions if p.isDir() and p.name.startswith(f\"{CONFIG.output.partition_column}=\")]\n",
    "        \n",
    "        if not dir_partitions:\n",
    "            print(\"   - No se encontraron particiones.\")\n",
    "            return\n",
    "\n",
    "        print(f\"   - Se encontraron {len(dir_partitions)} particiones.\")\n",
    "        for p in dir_partitions[:3]: # Mostramos las primeras 3\n",
    "            print(f\"     - {p.name}\")\n",
    "        \n",
    "        # Leer y mostrar una muestra de la primera partición\n",
    "        first_partition_path = dir_partitions[0].path\n",
    "        print(f\"\\n   - Leyendo muestra de la partición: {dir_partitions[0].name}\")\n",
    "        \n",
    "        df_sample = spark.read.format(CONFIG.output.format).load(first_partition_path)\n",
    "        df_sample.show(5, truncate=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        if \"java.io.FileNotFoundException\" in str(e):\n",
    "            print(f\"   - La carpeta de salida no existe o está vacía.\")\n",
    "        else:\n",
    "            print(f\"   - ❌ Error durante la auditoría: {e}\")\n",
    "\n",
    "# Ejecutar la auditoría en las dos carpetas de salida\n",
    "audit_output_folder(CONFIG.paths.output_processed, \"Datos Procesados (Silver)\")\n",
    "audit_output_folder(CONFIG.paths.output_quarantine, \"Datos en Cuarentena\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19599534-dc1a-42a2-9705-37c83f6e8952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusión de la Solución y Resumen de Hallazgos\n",
    "\n",
    "El pipeline desarrollado implementa una solución ETL completa y robusta, cumpliendo con todos los requisitos de la prueba técnica. Su diseño modular y parametrizable garantiza que la solución es mantenible, escalable y adaptable a futuras necesidades de negocio o cambios en las fuentes de datos.\n",
    "\n",
    "### Manejo de Calidad de Datos y Hallazgos\n",
    "\n",
    "Durante el análisis exploratorio, se identificaron varias irregularidades en los datos de origen:\n",
    "- **Tipos de entrega no especificados:** Presencia del código `COBR` que no correspondía a `rutina` ni a `bonificación`.\n",
    "- **Registros incompletos:** Filas con la columna `material` vacía.\n",
    "- **Datos duplicados:** Existencia de registros idénticos.\n",
    "- **Valores inválidos:** Precios con valor cero.\n",
    "\n",
    "Para manejar estos casos de forma sistemática, se implementó un **mecanismo de cuarentena**. En lugar de simplemente descartar los datos, los registros que no cumplen con las reglas de validación son desviados a una ruta de salida separada. Crucialmente, se les añade una columna (`rejection_reason`) que detalla el motivo exacto del rechazo. Este enfoque permite una auditoría posterior completa y evita la pérdida de información.\n",
    "\n",
    "### Características Destacadas de la Implementación\n",
    "\n",
    "Para asegurar la robustez y calidad de la solución, se incorporaron las siguientes prácticas:\n",
    "\n",
    "- **Diseño Orientado a Objetos:** La lógica ETL se encapsuló en la clase `DataPipeline`, promoviendo un código limpio, reutilizable y fácil de mantener.\n",
    "- **Definición Explícita de Esquema:** Se definió un esquema estricto (`StructType`) para la lectura de los datos. Esto mejora el rendimiento y previene errores de inferencia de tipos que podrían ocurrir en un entorno productivo.\n",
    "- **Estandarización y Enriquecimiento:** Se aplicó un estándar de nomenclatura consistente a todas las columnas finales y se enriquecieron los datos con información de valor, como el nombre completo del país y timestamps de procesamiento para auditoría.\n",
    "- **Orquestación Flexible:** La función `run_pipeline` actúa como un orquestador que permite ejecutar el proceso para múltiples escenarios con diferentes parámetros, demostrando la alta flexibilidad de la solución.\n",
    "\n",
    "### Consideraciones de Diseño\n",
    "\n",
    "En un entorno de producción, el archivo `config.yaml` se gestionaría como un artefacto independiente dentro del control de versiones.\n",
    "\n",
    "Para esta prueba técnica, se ha incluido la capacidad de generar y modificar la configuración de manera programática directamente desde el notebook. Esta decisión se tomó deliberadamente para demostrar un dominio avanzado de las herramientas (`dbutils`, `OmegaConf`, `PyYAML`) y para entregar un notebook interactivo y auto-contenido que permite probar diferentes escenarios sin necesidad de editar archivos manualmente."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "20250723_etl_entrega_productos",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}